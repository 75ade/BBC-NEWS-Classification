{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e46c94-d8d5-490c-9fad-e07c3446ebe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "159ecaa1-6efb-4bb3-b4ca-7be15d5320f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "895cfc0d-25bf-42fd-a00a-44a37866b9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3d1ae17-3a4d-438d-86ef-c858fa386dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.0.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b81c560a-c449-47bf-9456-0c3b3dfb395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f3b2cb-1a47-4710-bd39-8e9297ecb4f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk>=3.8->textblob) (4.67.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/626.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 626.3/626.3 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a19d8db-6fbe-4b2b-81a1-0088767f5d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "454f45ae-0666-48cd-83fb-d02c0ec273b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dallaglio his own man to the end\\n\\nControvers...</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Best person' for top legal job\\n\\nThe \"best pe...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Viewers to be able to shape TV\\n\\nImagine edit...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fox attacks Blair's Tory 'lies'\\n\\nTony Blair ...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Microsoft debuts security tools\\n\\nMicrosoft i...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2220</th>\n",
       "      <td>Michael film signals 'retirement'\\n\\nSinger Ge...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2221</th>\n",
       "      <td>Ray Charles studio becomes museum\\n\\nA museum ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2222</th>\n",
       "      <td>Chancellor rallies Labour voters\\n\\nGordon Bro...</td>\n",
       "      <td>politics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2223</th>\n",
       "      <td>Oscar nominees gear up for lunch\\n\\nLeonardo D...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2224</th>\n",
       "      <td>California sets fines for spyware\\n\\nThe maker...</td>\n",
       "      <td>tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2225 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text          topic\n",
       "0     Dallaglio his own man to the end\\n\\nControvers...          sport\n",
       "1     Best person' for top legal job\\n\\nThe \"best pe...       politics\n",
       "2     Viewers to be able to shape TV\\n\\nImagine edit...           tech\n",
       "3     Fox attacks Blair's Tory 'lies'\\n\\nTony Blair ...       politics\n",
       "4     Microsoft debuts security tools\\n\\nMicrosoft i...           tech\n",
       "...                                                 ...            ...\n",
       "2220  Michael film signals 'retirement'\\n\\nSinger Ge...  entertainment\n",
       "2221  Ray Charles studio becomes museum\\n\\nA museum ...  entertainment\n",
       "2222  Chancellor rallies Labour voters\\n\\nGordon Bro...       politics\n",
       "2223  Oscar nominees gear up for lunch\\n\\nLeonardo D...  entertainment\n",
       "2224  California sets fines for spyware\\n\\nThe maker...           tech\n",
       "\n",
       "[2225 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\DataScience\\csv\\bbc-dataset.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1bdc258-cba8-4bf2-9c2a-6b0d6f7a8d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text     0\n",
      "topic    0\n",
      "dtype: int64 \n",
      "\n",
      "['sport' 'politics' 'tech' 'business' 'entertainment'] \n",
      "Number of unique values: 5\n"
     ]
    }
   ],
   "source": [
    "#Check NaN values in the dataset and the unique values of topic column\n",
    "print(df.isna().sum(), '\\n')\n",
    "print(df['topic'].unique(), \"\\nNumber of unique values:\", len(df['topic'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e353cce7-9a6b-4a8f-b7f7-e982e9f2a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate rows:\n",
      "                                                    text     topic\n",
      "101   Super union' merger plan touted\\n\\nTwo of Brit...  politics\n",
      "200   Fox attacks Blair's Tory 'lies'\\n\\nTony Blair ...  politics\n",
      "316   Boothroyd calls for Lords speaker\\n\\nBetty Boo...  politics\n",
      "405   Bortolami predicts dour contest\\n\\nItaly skipp...     sport\n",
      "462   Economy 'strong' in election year\\n\\nUK busine...  business\n",
      "...                                                 ...       ...\n",
      "2184  Software watching while you work\\n\\nSoftware t...      tech\n",
      "2197  Blair backs 'pre-election budget'\\n\\nTony Blai...  politics\n",
      "2198  Hotspot users gain free net calls\\n\\nPeople us...      tech\n",
      "2204  File-swappers ready new network\\n\\nLegal attac...      tech\n",
      "2224  California sets fines for spyware\\n\\nThe maker...      tech\n",
      "\n",
      "[98 rows x 2 columns]\n",
      "Number of duplicate rows:\n",
      " 98\n"
     ]
    }
   ],
   "source": [
    "# Remove the duplicates in the dataset and display them\n",
    "\n",
    "duplicates = df.duplicated()\n",
    "\n",
    "# Display the rows that are duplicated\n",
    "print(\"Duplicate rows:\\n\", df[duplicates])\n",
    "\n",
    "# Count the number of duplicates:\n",
    "print(\"Number of duplicate rows:\\n\", df.duplicated().sum())\n",
    "\n",
    "# Remove the duplicate rows from the dataset\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d89a1bb-ce9d-4043-856c-835bbb2eebb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2127, 2)\n",
      "1701\n",
      "426\n",
      "(1701,)\n",
      "(426,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract features and labels:\n",
    "features = df['text']\n",
    "labels = df['topic']\n",
    "\n",
    "print(df.shape)\n",
    "\n",
    "# Split the data to train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, train_size=0.8, random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# print(y_train)\n",
    "# print(y_test)\n",
    "\n",
    "# Saved the X_test for the final predictions:\n",
    "X_test_origin = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d09046b-bac8-4d1d-b050-dbbeda867c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import libraries that can help to clean the data\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192f1a6-f7b7-4441-9350-95b4e5818804",
   "metadata": {},
   "source": [
    "Cleaning data process contains 6 steps:\n",
    "\n",
    "+Remove duplicate rows\n",
    "\n",
    "+Remove stop words in English (First time): a, an, the, then, after, before, ...\n",
    "\n",
    "+Remove non-English words\n",
    "\n",
    "+Lowercasing all the words\n",
    "\n",
    "+Removing punctuation and special characters: comma, dot, question mark, exclamtaion mark,...\n",
    "\n",
    "+Lemmatization: building, builds, built -> build\n",
    "\n",
    "+Remove stop words in English (Second time): a, an, the, then, after, before, ...\n",
    "\n",
    "+Use TF-IDF to calculate the weight of words, then base on the mean to remove words that are not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5df4d27-d7b0-44ab-a39d-20bf35376f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the English stop words\n",
    "df_stop_words = pd.read_csv(r\"C:\\Users\\Admin\\Downloads\\stop_words_english.csv\")\n",
    "# print(df_stop_words)\n",
    "\n",
    "new_stop_words = set(df_stop_words['stop_word'])\n",
    "# print(new_stop_words)\n",
    "\n",
    "#Get the list of English stop words\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# print(type(stop_words))\n",
    "\n",
    "# Define a function to remove the stop words\n",
    "def remove_stop_words(text):\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in new_stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c595cce-dcf0-46d2-9853-6d5034091496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "nltk.download('words')\n",
    "\n",
    "# Set of English words only\n",
    "english_words = set(words.words())\n",
    "\n",
    "# Define a function to remove non-English words\n",
    "def remove_non_english(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() in english_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "326b6112-49b6-47fe-825c-6bf8cc6d8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def correct_spelling(text):\n",
    "    return str(TextBlob(text).correct())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a08cf2-6c6a-4dec-891f-fcd0197bfd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1701,)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning data for training set\n",
    "# Remove stop words for X_train (First)\n",
    "X_train = X_train.apply(remove_stop_words)\n",
    "\n",
    "# Remove non-English words\n",
    "X_train = X_train.apply(remove_non_english)\n",
    "\n",
    "# # Check spell correction\n",
    "# X_train = X_train.apply(correct_spelling)\n",
    "\n",
    "# Lowercasing all the words\n",
    "X_train = X_train.str.lower()\n",
    "\n",
    "# Removing punctuation and special characters\n",
    "X_train = X_train.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "X_train = X_train.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Remove stop words for X_train (Second)\n",
    "X_train = X_train.apply(remove_stop_words)\n",
    "\n",
    "# X_train.to_csv(\"temp_file.csv\", index=False)\n",
    "\n",
    "# X_train.to_csv(\"temp_file.csv\", index=False)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "914c5284-873b-4cc6-b7e4-bdfde4301310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426,)\n"
     ]
    }
   ],
   "source": [
    "# Cleaning data for test set\n",
    "\n",
    "# Remove stop words for X_test (First)\n",
    "X_test = X_test.apply(remove_stop_words)\n",
    "\n",
    "# Remove non-English words\n",
    "X_test = X_test.apply(remove_non_english)\n",
    "\n",
    "# Lowering all the words\n",
    "X_test = X_test.str.lower()\n",
    "\n",
    "# Removing punctuation and special characters\n",
    "X_test = X_test.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "\n",
    "# Lemmatization\n",
    "X_test = X_test.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "# Remove stop words for X_test (Second)\n",
    "X_test = X_test.apply(remove_stop_words)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f905da9c-87be-4bef-bbec-fa12c44e3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5c9230b-4a2a-45f8-84e9-da1e982d0641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1701, 10764)\n",
      "(1701, 2437)\n"
     ]
    }
   ],
   "source": [
    "# Apply TF-IDF to train set X_train\n",
    "matrix_tfidf_train = vec.fit_transform(X_train)\n",
    "\n",
    "df_tfdif_train = pd.DataFrame(matrix_tfidf_train.toarray(), columns=vec.get_feature_names_out())\n",
    "\n",
    "mean_tfidf = df_tfdif_train.mean()\n",
    "# print(\"Mean TF-IDF values:\\n\", mean_tfidf)\n",
    "threshold = mean_tfidf.mean()\n",
    "\n",
    "important_terms = mean_tfidf[mean_tfidf > threshold].index.tolist()\n",
    "# print(\"\\nTerms to keep:\", important_terms)\n",
    "\n",
    "X_train_filtered = df_tfdif_train[important_terms]\n",
    "\n",
    "print(df_tfdif_train.shape)\n",
    "print(X_train_filtered.shape)\n",
    "\n",
    "# print(mean_tfidf)\n",
    "# print(\"\\nDone here\")\n",
    "\n",
    "# print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4175f9f6-ff1a-49be-9118-0f24458d9260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 10764)\n",
      "(426, 2437)\n"
     ]
    }
   ],
   "source": [
    "# Apply TF-IDF to test set (X_test)\n",
    "matrix_tfidf_test = vec.transform(X_test)\n",
    "\n",
    "df_tfidf_test = pd.DataFrame(matrix_tfidf_test.toarray(), columns=vec.get_feature_names_out())\n",
    "\n",
    "X_test_filtered = df_tfidf_test[important_terms]\n",
    "\n",
    "print(df_tfidf_test.shape)\n",
    "print(X_test_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40cb76b3-3b66-46e3-ba80-ba662194a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_filtered.to_csv(\"X_train_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f33a3793-5bb7-4a0f-9cc6-6f74f36a805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.DataFrame(y_train)\n",
    "df_temp.to_csv(\"y_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f298192-1fa5-46c7-b5b4-6fcddd38d56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filtered.to_csv(\"X_test_filtered.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef451887-3f71-480a-8f42-e481772190e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_1 = pd.DataFrame(y_test)\n",
    "df_temp_1.to_csv(\"y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fcc80247-4b4a-4cd4-9e50-5e250b7ac1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_origin.to_csv(\"X_test_origin.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35cbe02-ade7-4efd-affe-dcac3b23c122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
